{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2  -  Deep Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#reproducibility\n",
    "np.random.seed(12345)\n",
    "\n",
    "perc_train = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 3000\n",
      "Length of a key: 7\n",
      "number of digits: 9\n"
     ]
    }
   ],
   "source": [
    "#import datafile\n",
    "fname = 'secretkeys_exe.csv'\n",
    "dataset = np.loadtxt(fname, delimiter=',', dtype=int)\n",
    "N = len(dataset)\n",
    "print(\"Length of the dataset:\", N)\n",
    "\n",
    "L = len(str(dataset[0][0]))\n",
    "D = 9     #digits\n",
    "print(\"Length of a key:\", L)\n",
    "print(\"number of digits:\", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding\n",
    "def expand(S):   \n",
    "    if (len(str(S))!=L):\n",
    "        print('mismatch!')\n",
    "        return []\n",
    "    x = np.zeros(L*D, dtype=int)\n",
    "    p = 10**(L-1)\n",
    "    j = 0\n",
    "    while j<L:\n",
    "        q = int(S/p)    #first digit\n",
    "        # 1 to 9 --> 0 to 8, that's why q-1 in the following line\n",
    "        x[j*D+(q-1)] = 1\n",
    "        j += 1\n",
    "        S = S - q*p\n",
    "        p = int(p/10)\n",
    "    return x\n",
    "\n",
    "x_all = [None]*N \n",
    "for i in range(N):\n",
    "    x_all[i] = expand(dataset[:,0][i])\n",
    "x_all = np.array(x_all)\n",
    "y_all = np.array(dataset[:,-1])\n",
    "\n",
    "#Splitting in train and test set\n",
    "N_train = int(perc_train*N)\n",
    "x_train, y_train = x_all[:N_train], y_all[:N_train]\n",
    "x_test , y_test  = x_all[N_train:], y_all[N_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing the architecture of the network\n",
    "model = Sequential(name='keys_1')   #sequence of dense layers\n",
    "model.add(Dense(L*D, input_shape=(L*D,), activation='relu'))\n",
    "model.add(Dense(max(10, int(L*D/2)), activation='relu'))  #layer with half of the nodes and >= 10\n",
    "model.add(Dense(max(6, int(L*D/4)), activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.4)) #removing some nodes\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "fit = model.fit(x_train, y_train, epochs=80, batch_size=20, validation_data=(x_test, y_test), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "# accuracy for training and validation(test)\n",
    "ax1.plot(fit.history['accuracy'])\n",
    "ax1.plot(fit.history['val_accuracy'])\n",
    "ax1.set_title('Model accuracy')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend(['Train', 'Validation'])\n",
    "\n",
    "# loss for training and validation(test)\n",
    "ax2.plot(fit.history['loss'])\n",
    "ax2.plot(fit.history['val_loss'])\n",
    "ax2.set_title('Model loss')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend(['Train', 'Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max accuracy up to 20 epochs:', np.array(fit.history['val_accuracy'][:20]).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dataset = np.zeros( (N*L, 2), dtype=int )\n",
    "\n",
    "# rotating the digits of every key\n",
    "for key,ind in zip(dataset, range(len(dataset))):\n",
    "    for i in range(L):\n",
    "        L_dataset[ind*L+i][0] = int( str(key[0])[i:]+str(key[0])[:i] )\n",
    "        L_dataset[ind*L+i][1] = key[1]\n",
    "\n",
    "print(\"Original shape of dataset:\", dataset.shape)\n",
    "print(\"New shape of dataset:\", L_dataset.shape)\n",
    "\n",
    "np.random.shuffle(L_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_x_all = [None]*(N*L)\n",
    "for i in range(N*L):\n",
    "    L_x_all[i] = expand(L_dataset[:,0][i])\n",
    "L_x_all = np.array(L_x_all)\n",
    "L_y_all = np.array(L_dataset[:,-1])\n",
    "\n",
    "#Splitting in train and test set\n",
    "N_train = int(perc_train*N*L)\n",
    "L_x_train, L_y_train = L_x_all[:N_train], L_y_all[:N_train]\n",
    "L_x_test, L_y_test   = L_x_all[N_train:], L_y_all[N_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing the architecture of the network\n",
    "model = Sequential(name='keys_larger')   #sequence of dense layers\n",
    "model.add(Dense(L*D, input_shape=(L*D,), activation='relu'))\n",
    "model.add(Dense(max(10, int(L*D/2)), activation='relu'))  #layer with half of the nodes and >= 10\n",
    "model.add(Dense(max(6, int(L*D/4)), activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2)) #removing some nodes\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Fitting again with more data\n",
    "fit = model.fit(L_x_train, L_y_train, epochs=80, batch_size=20, validation_data=(L_x_test, L_y_test), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "# accuracy for training and validation(test)\n",
    "ax1.plot(fit.history['accuracy'])\n",
    "ax1.plot(fit.history['val_accuracy'])\n",
    "ax1.set_title('Model accuracy')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend(['Train', 'Validation'])\n",
    "\n",
    "# loss for training and validation(test)\n",
    "ax2.plot(fit.history['loss'])\n",
    "ax2.plot(fit.history['val_loss'])\n",
    "ax2.set_title('Model loss')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend(['Train', 'Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max accuracy up to 20 epochs:', np.array(fit.history['val_accuracy'][:20]).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Grid search over hyper-parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "\n",
    "def compile_model(optimizer=optimizers.Adam(),activation='relu',dropout_rate=0.2):\n",
    "    # create the mode\n",
    "    mod = Sequential(name='keys_larger') \n",
    "    mod.add(Dense(L*D, input_shape=(L*D,), activation=activation))\n",
    "    mod.add(Dense(max(10, int(L*D/2)), activation=activation))  #layer with half of the nodes and >= 10\n",
    "    mod.add(Dense(max(6, int(L*D/4)), activation=activation))\n",
    "\n",
    "    mod.add(Dropout(dropout_rate)) #removing some nodes\n",
    "    mod.add(Dense(1, activation='sigmoid'))\n",
    "    # compile the model\n",
    "    mod.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "# call Keras scikit wrapper\n",
    "model_gridsearch = KerasClassifier(build_fn=compile_model, \n",
    "                        epochs=50, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=False)\n",
    "\n",
    "# list of allowed optional arguments for the optimizer, see `compile_model()`\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "\n",
    "# define parameter dictionary\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "# call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=4, cv=4)\n",
    "grid_result = grid.fit(x_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "best_optimizer = grid_result.best_params_['optimizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "# call Keras scikit wrapper\n",
    "model_gridsearch = KerasClassifier(build_fn=compile_model, \n",
    "                        epochs=50, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=False)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40]\n",
    "\n",
    "# define parameter dictionary\n",
    "param_grid = dict(batch_size=batch_size)\n",
    "# call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=4)\n",
    "grid_result = grid.fit(x_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "best_batch_size = grid_result.best_params_['batch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "# call Keras scikit wrapper\n",
    "model_gridsearch = KerasClassifier(build_fn=compile_model, \n",
    "                        epochs=50, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=False)\n",
    "\n",
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softsign', 'relu', 'tanh', 'sigmoid', 'linear']\n",
    "# define parameter dictionary\n",
    "param_grid = dict(activation=activation)\n",
    "\n",
    "# call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=4, cv=4)\n",
    "grid_result = grid.fit(x_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "best_activation = grid_result.best_params_['activation']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "# call Keras scikit wrapper\n",
    "model_gridsearch = KerasClassifier(build_fn=compile_model, \n",
    "                        epochs=50, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=False)\n",
    "\n",
    "# define the grid search parameters\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# define parameter dictionary\n",
    "param_grid = dict(dropout_rate=dropout_rate)\n",
    "\n",
    "# call scikit grid search module\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=4, cv=4)\n",
    "grid_result = grid.fit(x_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "best_dropout_rate = grid_result.best_params_['dropout_rate']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = Sequential(name='keys_larger') \n",
    "\n",
    "activation = best_activation\n",
    "mod.add(Dense(L*D, input_shape=(L*D,), activation=activation))\n",
    "mod.add(Dense(max(10, int(L*D/2)), activation=activation))  #layer with half of the nodes and >= 10\n",
    "mod.add(Dense(max(6, int(L*D/4)), activation=activation))\n",
    "\n",
    "dropout_rate = best_dropout_rate\n",
    "mod.add(Dropout(dropout_rate)) #removing some nodes\n",
    "mod.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = best_optimizer\n",
    "# compile the model\n",
    "mod.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "batch_size = best_batch_size\n",
    "epochs = 40\n",
    "#Fitting again with more data\n",
    "fit = mod.fit(L_x_train, L_y_train, epochs=epochs, batch_size=batch_size, validation_data=(L_x_test, L_y_test), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "# accuracy for training and validation(test)\n",
    "ax1.plot(fit.history['accuracy'])\n",
    "ax1.plot(fit.history['val_accuracy'])\n",
    "ax1.set_title('Model accuracy')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend(['Train', 'Validation'])\n",
    "\n",
    "# loss for training and validation(test)\n",
    "ax2.plot(fit.history['loss'])\n",
    "ax2.plot(fit.history['val_loss'])\n",
    "ax2.set_title('Model loss')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend(['Train', 'Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max accuracy up to 20 epochs:', np.array(fit.history['val_accuracy'][:20]).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Data rescaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_x_rescale = (L_x_all - L_x_all.mean()) / L_x_all.std()\n",
    "\n",
    "#Splitting in train and test set\n",
    "N_train = int(perc_train*N*L)\n",
    "L_x_train, L_y_train = L_x_rescale[:N_train], L_y_all[:N_train]\n",
    "L_x_test, L_y_test   = L_x_rescale[N_train:], L_y_all[N_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = Sequential(name='keys_larger') \n",
    "\n",
    "activation = best_activation\n",
    "mod.add(Dense(L*D, input_shape=(L*D,), activation=activation))\n",
    "mod.add(Dense(max(10, int(L*D/2)), activation=activation))  #layer with half of the nodes and >= 10\n",
    "mod.add(Dense(max(6, int(L*D/4)), activation=activation))\n",
    "\n",
    "dropout_rate = best_dropout_rate\n",
    "mod.add(Dropout(dropout_rate)) #removing some nodes\n",
    "mod.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = best_optimizer\n",
    "# compile the model\n",
    "mod.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "batch_size = best_batch_size\n",
    "epochs = 100\n",
    "#Fitting again with more data\n",
    "fit = mod.fit(L_x_train, L_y_train, epochs=epochs, batch_size=batch_size, validation_data=(L_x_test, L_y_test), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,6))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "# accuracy for training and validation(test)\n",
    "ax1.plot(fit.history['accuracy'])\n",
    "ax1.plot(fit.history['val_accuracy'])\n",
    "ax1.set_title('Model accuracy')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend(['Train', 'Validation'])\n",
    "\n",
    "# loss for training and validation(test)\n",
    "ax2.plot(fit.history['loss'])\n",
    "ax2.plot(fit.history['val_loss'])\n",
    "ax2.set_title('Model loss')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend(['Train', 'Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max accuracy up to 20 epochs:', np.array(fit.history['val_accuracy'][:20]).max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
